# ğŸ§  PerceptrÃ³n - Compuerta LÃ³gica OR

## ğŸ“‹ DescripciÃ³n
ImplementaciÃ³n de una red neuronal monocapa (PerceptrÃ³n) para simular el comportamiento de una compuerta lÃ³gica OR. Este proyecto demuestra cÃ³mo una neurona artificial puede aprender patrones linealmente separables mediante el algoritmo de aprendizaje del perceptrÃ³n.

## ğŸ¯ Objetivo
Entrenar un perceptrÃ³n para que aprenda la funciÃ³n lÃ³gica OR, donde la salida es 1 si al menos una de las entradas es 1, y 0 solo cuando ambas entradas son 0.

---

## ğŸ”§ Requisitos

### LibrerÃ­as necesarias:
```python
numpy        # Operaciones matemÃ¡ticas y manejo de arrays
matplotlib   # VisualizaciÃ³n de grÃ¡ficos
```

### InstalaciÃ³n:
```bash
pip install numpy matplotlib
```

---

## ğŸ“Š Tabla de Verdad - Compuerta OR

| xâ‚ | xâ‚‚ | Salida |
|----|----|--------|
| 0  | 0  | 0      |
| 0  | 1  | 1      |
| 1  | 0  | 1      |
| 1  | 1  | 1      |

---

## ğŸ—ï¸ Estructura del CÃ³digo

### **1. ImportaciÃ³n de LibrerÃ­as**
```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
```
- **numpy**: Manejo eficiente de arrays y operaciones matemÃ¡ticas
- **matplotlib**: CreaciÃ³n de grÃ¡ficos y visualizaciones
- **ListedColormap**: PersonalizaciÃ³n de colores en regiones de decisiÃ³n

---

### **2. Clase Perceptron**

#### **MÃ©todo `__init__(self, eta=0.01, epochs=60)`**
**Constructor de la clase**
- **eta**: Tasa de aprendizaje (learning rate)
  - Controla quÃ© tan rÃ¡pido aprende el modelo
  - Valor tÃ­pico: 0.01 - 0.1
- **epochs**: NÃºmero de Ã©pocas de entrenamiento
  - Una Ã©poca = un recorrido completo sobre todos los datos
- **w**: Vector de pesos (inicializado en `None`)
- **errors**: Lista para almacenar errores por Ã©poca

```python
def __init__(self, eta=0.01, epochs=60):
    self.eta = eta
    self.epochs = epochs
    self.w = None
    self.errors = []
```

---

#### **MÃ©todo `fit(self, X, y)`**
**Entrena el perceptrÃ³n con los datos**

**Pasos:**
1. **InicializaciÃ³n de pesos**: `self.w = np.zeros(1 + X.shape[1])`
   - Crea un vector de pesos con ceros
   - TamaÃ±o: 1 (bias) + nÃºmero de caracterÃ­sticas
   - Para OR: [bias, wâ‚, wâ‚‚] = [0, 0, 0]

2. **Bucle de Ã©pocas**: Repite el entrenamiento `epochs` veces

3. **Por cada ejemplo de entrenamiento**:
   - **PredicciÃ³n**: Calcula la salida del perceptrÃ³n
   - **CÃ¡lculo del error**: `update = eta * (target - prediction)`
   - **ActualizaciÃ³n de pesos**:
     - `w[1:] += update * xi` â†’ Actualiza pesos de caracterÃ­sticas
     - `w[0] += update` â†’ Actualiza bias
   - **Conteo de errores**: Suma 1 si hubo error

4. **Almacenamiento**: Guarda el nÃºmero de errores de la Ã©poca

```python
def fit(self, X, y):
    self.w = np.zeros(1 + X.shape[1])
    self.errors = []
    
    for epoch in range(self.epochs):
        errors = 0
        for xi, target in zip(X, y):
            prediction = self.predict(xi)
            update = self.eta * (target - prediction)
            self.w[1:] += update * xi
            self.w[0] += update
            errors += int(update != 0.0)
        self.errors.append(errors)
    return self
```

---

#### **MÃ©todo `net_input(self, X)`**
**Calcula la entrada neta (suma ponderada)**

**FÃ³rmula**: z = wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + bias

```python
def net_input(self, X):
    return np.dot(X, self.w[1:]) + self.w[0]
```

**Ejemplo**:
- Si w = [0.5, 0.3, 0.2] y x = [1, 0]
- z = (0.3 Ã— 1) + (0.2 Ã— 0) + 0.5 = 0.8

---

#### **MÃ©todo `predict(self, X)`**
**FunciÃ³n de activaciÃ³n (escalÃ³n unitario)**

**Regla**:
- Si z â‰¥ 0 â†’ Salida = 1
- Si z < 0 â†’ Salida = 0

```python
def predict(self, X):
    return np.where(self.net_input(X) >= 0, 1, 0)
```

---

### **3. Datos de Entrenamiento**

```python
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 1, 1, 1])
```

- **X**: Matriz de caracterÃ­sticas (entradas)
- **y**: Vector de etiquetas (salidas esperadas)

---

### **4. Entrenamiento**

```python
ppn = Perceptron(eta=0.1, epochs=10)
ppn.fit(X, y)
```

- **eta=0.1**: Tasa de aprendizaje moderada
- **epochs=10**: Suficiente para converger en OR

---

### **5. Visualizaciones**

#### **GrÃ¡fico 1: EvoluciÃ³n del Error**
Muestra cÃ³mo disminuyen los errores durante el entrenamiento

**CaracterÃ­sticas**:
- LÃ­nea azul con marcadores
- Ãrea sombreada bajo la curva
- Grid para mejor lectura

#### **GrÃ¡fico 2: Frontera de DecisiÃ³n**
Visualiza cÃ³mo el perceptrÃ³n separa las clases

**Elementos**:
- **Regiones de color**: Representan las predicciones en todo el espacio
- **LÃ­nea negra discontinua**: Frontera de decisiÃ³n (hiperplano)
- **Puntos rojos (X)**: Salida = 0
- **Puntos verdes (O)**: Salida = 1
- **Anotaciones**: Coordenadas de cada punto

---

## ğŸ§® MatemÃ¡ticas del PerceptrÃ³n

### **EcuaciÃ³n de la recta separadora**
```
wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + bias = 0
```

### **Regla de actualizaciÃ³n**
```
Î”w = Î· Ã— (y_real - y_predicha) Ã— x
```

Donde:
- **Î· (eta)**: Tasa de aprendizaje
- **y_real**: Etiqueta correcta
- **y_predicha**: PredicciÃ³n del modelo
- **x**: Vector de entrada

### **CondiciÃ³n de convergencia**
El perceptrÃ³n converge si los datos son **linealmente separables** (existe una lÃ­nea que separa las clases). La compuerta OR cumple esta condiciÃ³n.

---

## ğŸš€ CÃ³mo Ejecutar

1. **Guardar el cÃ³digo** como `perceptron_or.py`

2. **Ejecutar desde terminal**:
```bash
python perceptron_or.py
```

3. **Ejecutar desde PowerShell** (Windows):
```powershell
python "PERCEPTRON PARA COMPUERTA OR.py"
```

---

## ğŸ“ˆ Resultados Esperados

### **Salida en Consola**:
```
==================================================
COMPUERTA OR - PERCEPTRÃ“N
==================================================

Pesos finales:
w1 = 0.300, w2 = 0.300, bias = -0.100

Tabla de verdad - Predicciones:
--------------------------------------------------
Entrada [x1, x2] â†’ PredicciÃ³n | Real
--------------------------------------------------
Entrada: [0 0] â†’ PredicciÃ³n: 0, Real: 0 âœ“
Entrada: [0 1] â†’ PredicciÃ³n: 1, Real: 1 âœ“
Entrada: [1 0] â†’ PredicciÃ³n: 1, Real: 1 âœ“
Entrada: [1 1] â†’ PredicciÃ³n: 1, Real: 1 âœ“

==================================================
RESUMEN - COMPUERTA OR
==================================================
âœ“ La compuerta OR devuelve 1 si AL MENOS UNA entrada es 1
âœ“ Solo devuelve 0 cuando AMBAS entradas son 0

ğŸ“Š Ã‰pocas necesarias para converger: 10
ğŸ“Š Errores finales: 0
ğŸ“Š PrecisiÃ³n: 100%

ğŸ”§ EcuaciÃ³n de la recta separadora:
   0.300Â·xâ‚ + 0.300Â·xâ‚‚ + -0.100 = 0
==================================================
```

### **GrÃ¡ficos Generados**:
1. Curva de aprendizaje (errores vs Ã©pocas)
2. Frontera de decisiÃ³n con puntos de datos

---

## ğŸ” Conceptos Clave

### **PerceptrÃ³n**
- Primera neurona artificial (1958 - Frank Rosenblatt)
- Clasificador binario lineal
- Base de las redes neuronales modernas

### **Limitaciones**
- âŒ No puede resolver problemas no lineales (ej: XOR)
- âœ… Perfecto para problemas linealmente separables (AND, OR)

### **Ventajas**
- âœ… Simple y rÃ¡pido
- âœ… Garantiza convergencia si es linealmente separable
- âœ… FÃ¡cil de interpretar

---

## ğŸ“š Referencias

- Rosenblatt, F. (1958). "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"
- Bishop, C. M. (2006). "Pattern Recognition and Machine Learning"
- Raschka, S. & Mirjalili, V. (2019). "Python Machine Learning"

---

## ğŸ‘¨â€ğŸ’» Autor
**XAVIER-801**

## ğŸ“„ Licencia
Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos.

---

## ğŸ¤ Contribuciones
Â¡Las contribuciones son bienvenidas! Si encuentras errores o tienes sugerencias:
1. Fork el repositorio
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

---

## ğŸ“ Contacto
+51 936446054 / xavierpachacutec.801.x@gmail.com
---

## ğŸ“ Uso Educativo
Este cÃ³digo estÃ¡ diseÃ±ado para:
- Estudiantes de Inteligencia Artificial
- Cursos de Machine Learning
- Talleres de Redes Neuronales
- Proyectos acadÃ©micos

**Curso**: Inteligencia Artificial
**Tema**: Redes Neuronales Monocapa

---

## âœ¨ PrÃ³ximos Pasos
- [ ] Implementar compuerta AND
- [ ] Implementar compuerta NAND
- [ ] Demostrar por quÃ© XOR no funciona
- [ ] Crear perceptrÃ³n multicapa (MLP)
- [ ] AÃ±adir validaciÃ³n cruzada
- [ ] Implementar early stopping
